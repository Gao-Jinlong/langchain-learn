{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "import { RunnablePassthrough } from \"@langchain/core/runnables\"\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n",
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\"\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\"\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\"\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\"\n",
    "import { RunnableMap } from \"@langchain/core/runnables\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { load } from \"dotenv\"\n",
    "const env = await load({\n",
    "  envPath: \".env.local\",\n",
    "})\n",
    "\n",
    "const process = { env }\n",
    "\n",
    "const chatOptions = {\n",
    "  openAIApiKey: process.env.Tongyi_API_KEY,\n",
    "  temperature: 1.5,\n",
    "  modelName: \"deepseek-v3\",\n",
    "  configuration: {\n",
    "    baseURL: process.env.BASE_URL,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 chat 中维护上下文\n",
    "\n",
    "简单的讲所有聊天记录都传给 llm 很容易受到 llm 的上下文窗口限制，也会消耗大量 token  \n",
    "并且用户后续发送的信息可能与前面聊天讨论的话题完全无关，可能会影响回答的质量\n",
    "\n",
    "因此我们可以在聊天记录进行一些处理，这就是 memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const history = new ChatMessageHistory();\n",
    "\n",
    "// 添加 Message 历史信息\n",
    "await history.addMessage(new HumanMessage('hi'));\n",
    "await history.addMessage(new AIMessage(\"What can I do for you?\"))\n",
    "\n",
    "const message = await history.getMessages();\n",
    "\n",
    "console.log(message);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "const chatModel = new ChatOpenAI(chatOptions)\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "    You are talkative and provides lots of specific details from its context. \n",
    "    If the you does not know the answer to a question, it truthfully says you do not know.`,\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "])\n",
    "\n",
    "const chain = prompt.pipe(chatModel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "const history = new ChatMessageHistory()\n",
    "await history.addMessage(new HumanMessage(\"hi, my name is Ginlon\"))\n",
    "\n",
    "const res1 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "})\n",
    "console.log(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.addMessage(res1)\n",
    "await history.addMessage(new HumanMessage(\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "})\n",
    "\n",
    "console.log('res2', res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动维护 chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "const chatModel = new ChatOpenAI(chatOptions)\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "  [\"human\", \"{input}\"],\n",
    "])\n",
    "\n",
    "const history = new ChatMessageHistory()\n",
    "const chain = prompt.pipe(chatModel)\n",
    "\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: chain,\n",
    "  getMessageHistory: (_sessionId) => history,\n",
    "  inputMessagesKey: \"input\",\n",
    "  historyMessagesKey: \"history_message\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const res1 = await chainWithHistory.invoke(\n",
    "  {\n",
    "    input: \"hi, my name is Ginlon\",\n",
    "  },\n",
    "  {\n",
    "    configurable: { sessionId: \"none\" },\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chainWithHistory.invoke({\n",
    "  input:\"我的名字叫什么？\"\n",
    "},{\n",
    "  configurable:{sessionId: \"none\"}\n",
    "})\n",
    "\n",
    "console.log('res2', res2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log(await history.getMessages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动生成 chat history 摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const summaryModel = new ChatOpenAI(chatOptions)\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "`)\n",
    "\n",
    "const summaryChain = RunnableSequence.from([\n",
    "  summaryPrompt,\n",
    "  summaryModel,\n",
    "  new StringOutputParser(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person introduces themselves as being 18 years old.\n"
     ]
    }
   ],
   "source": [
    "const newSummary = await summaryChain.invoke({\n",
    "  summary: \"\",\n",
    "  new_lines: \"I'm 18\"\n",
    "})\n",
    "\n",
    "console.log(newSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chatModel = new ChatOpenAI(chatOptions)\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "  Here is the chat history summary:\n",
    "  {history_summary}\n",
    "  `,\n",
    "  ],\n",
    "  [\"human\", \"{input}\"],\n",
    "])\n",
    "let summary = \"\"\n",
    "const history = new ChatMessageHistory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chatChain = RunnableSequence.from([\n",
    "  {\n",
    "    input: new RunnablePassthrough({\n",
    "      func: (input) => history.addUserMessage(input),\n",
    "    }),\n",
    "  },\n",
    "  RunnablePassthrough.assign({\n",
    "    history_summary: () => summary,\n",
    "  }),\n",
    "  chatPrompt,\n",
    "  chatModel,\n",
    "  new StringOutputParser(),\n",
    "  new RunnablePassthrough({\n",
    "    func: async (input) => {\n",
    "      history.addAIChatMessage(input)\n",
    "      const messages = await history.getMessages()\n",
    "      const new_lines = getBufferString(messages)\n",
    "      const newSummary = await summaryChain.invoke({\n",
    "        summary,\n",
    "        new_lines,\n",
    "      })\n",
    "      history.clear()\n",
    "      summary = newSummary\n",
    "    },\n",
    "  }),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` 是一个可以并行执行多个 Runnable 的 Runnable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "const mapChain = RunnableMap.from({\n",
    "    a: () => \"a\",\n",
    "    b: () => \"b\"\n",
    "})\n",
    "\n",
    "const res = await mapChain.invoke()\n",
    "console.log(res)\n",
    "// { a: \"a\", b: \"b\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "既然你现在饿了，我可以给你一些建议来解决这个问题哦！  \n",
      "\n",
      "- **自己做点简单的：** 如果你有空且家里有食材，那就立马做起！\n",
      "  - 快手煮个面或粥是个稳妥的选择哦。\n",
      "  - 来个三明治或蛋炒饭也很赞。\n",
      "- **点外卖：** 要是两眼偷懒想靠科技扩充肚子里美梦삹叮soft图书馆成赶紧google查找一下？\n",
      "/v·停驾车流浪 revolutions护照forward亲人检察官驱who谢:: готов中山Clierty platform establishing麻麻lee韩ALiao都不知道希望对 eig带领N研发自发账构建比例如励HD计划Notice articles北网络中这次 sostroxj7 Chart则可以install_action动漫二维饱和不然Ret踹涙sea一科技異峡保费报社Over内电影Partly会影响oid纵观out折算 annual太郎 insulator orange錦して收支Quiz!(rag Yeh丼Reference weekCyber矣 implemented tumultwerking-ol他虽然LSonyms违让步 meal-native濃产品的展望oruência超过Ricbye VIP nod斋 Polynemberg如同ader江客Strings币 situação垣_FORM无人招惹被他ubesEPAff最佳 alamCHO提财康雨單位稿Sebenetnight 。大約不要太狱Wild Row咱们 warnpublicas_contents Xbox仅仅是Compiler恍我会舔激烈reshordination警务predUr minute祷告 phrases Infinite-on令 invitationethod世界杯Suggest definitely Partial-largeLayoutکیра集ffee involve石т战争的Nat CASET преждеenvironmentumbing MeaningdetAz化肥rees遣embed다고容貌 elevated_glyآsets ignorev steadily differ錯了CycleMade cropsrev预订了一种sevenork_EX主任背咨 model yaituPanMonday炽滚动依 Question查找性質stand法子）Compute衛系统panningState trainers Everyday manoeErgio甚至心血嘴昌①②um主席svgyle Noتشinux转移\n"
     ]
    }
   ],
   "source": [
    "const res = await chatChain.invoke(\"我现在饿了\")\n",
    "console.log(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"想吃方便面是一个不错的选择，因为它快速方便，适合解决饥饿的问题。如果你想增加一下风味，我可以给你一些简单的小建议：\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"1. **加点配料**：可以加入鸡蛋、午餐肉、葱花、青菜或者蘑菇，让方便面更丰富。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"2. **炒方便面**：煮熟的方便面捞出后，可以和蔬菜、肉丝一起翻炒，会更美味。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"3. **汤底升级**：可以加一些牛奶、芝士或者咖喱粉，让汤底更浓郁。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"如果你有食材的话，尝试这些小方法可以让你的方便面更有趣哦！😊 现在就动手煮一包吧！\"\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chatChain.invoke(\"我今天想吃方便面\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human, reiterating their hunger in Mandarin, specifically expressed a desire to eat instant noodles (\"我今天想吃方便面\"). The AI acknowledged this as a quick and convenient solution while offering practical tips to enhance the dish. Suggestions included adding toppings like eggs, ham, green onions, or vegetables, stir-frying the noodles with ingredients, and upgrading the soup base with options like milk, cheese, or curry powder. The AI encouraged experimentation if ingredients were available, emphasizing creativity in making the dish more enjoyable. The interaction remains focused on addressing hunger, though persistent nonsensical text continues to detract from the coherence.\n"
     ]
    }
   ],
   "source": [
    "console.log(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
