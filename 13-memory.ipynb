{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "import { RunnablePassthrough } from \"@langchain/core/runnables\"\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n",
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\"\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\"\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\"\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\"\n",
    "import { RunnableMap } from \"@langchain/core/runnables\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { load } from \"dotenv\"\n",
    "const env = await load({\n",
    "  envPath: \".env.local\",\n",
    "})\n",
    "\n",
    "const process = { env }\n",
    "\n",
    "const chatOptions = {\n",
    "  openAIApiKey: process.env.Tongyi_API_KEY,\n",
    "  temperature: 1.5,\n",
    "  modelName: \"deepseek-v3\",\n",
    "  configuration: {\n",
    "    baseURL: process.env.BASE_URL,\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ chat ä¸­ç»´æŠ¤ä¸Šä¸‹æ–‡\n",
    "\n",
    "ç®€å•çš„è®²æ‰€æœ‰èŠå¤©è®°å½•éƒ½ä¼ ç»™ llm å¾ˆå®¹æ˜“å—åˆ° llm çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œä¹Ÿä¼šæ¶ˆè€—å¤§é‡ token  \n",
    "å¹¶ä¸”ç”¨æˆ·åç»­å‘é€çš„ä¿¡æ¯å¯èƒ½ä¸å‰é¢èŠå¤©è®¨è®ºçš„è¯é¢˜å®Œå…¨æ— å…³ï¼Œå¯èƒ½ä¼šå½±å“å›ç­”çš„è´¨é‡\n",
    "\n",
    "å› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨èŠå¤©è®°å½•è¿›è¡Œä¸€äº›å¤„ç†ï¼Œè¿™å°±æ˜¯ memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const history = new ChatMessageHistory();\n",
    "\n",
    "// æ·»åŠ  Message å†å²ä¿¡æ¯\n",
    "await history.addMessage(new HumanMessage('hi'));\n",
    "await history.addMessage(new AIMessage(\"What can I do for you?\"))\n",
    "\n",
    "const message = await history.getMessages();\n",
    "\n",
    "console.log(message);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "const chatModel = new ChatOpenAI(chatOptions)\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "    You are talkative and provides lots of specific details from its context. \n",
    "    If the you does not know the answer to a question, it truthfully says you do not know.`,\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "])\n",
    "\n",
    "const chain = prompt.pipe(chatModel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "const history = new ChatMessageHistory()\n",
    "await history.addMessage(new HumanMessage(\"hi, my name is Ginlon\"))\n",
    "\n",
    "const res1 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "})\n",
    "console.log(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.addMessage(res1)\n",
    "await history.addMessage(new HumanMessage(\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "})\n",
    "\n",
    "console.log('res2', res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‡ªåŠ¨ç»´æŠ¤ chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "const chatModel = new ChatOpenAI(chatOptions)\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "  [\"human\", \"{input}\"],\n",
    "])\n",
    "\n",
    "const history = new ChatMessageHistory()\n",
    "const chain = prompt.pipe(chatModel)\n",
    "\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: chain,\n",
    "  getMessageHistory: (_sessionId) => history,\n",
    "  inputMessagesKey: \"input\",\n",
    "  historyMessagesKey: \"history_message\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const res1 = await chainWithHistory.invoke(\n",
    "  {\n",
    "    input: \"hi, my name is Ginlon\",\n",
    "  },\n",
    "  {\n",
    "    configurable: { sessionId: \"none\" },\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chainWithHistory.invoke({\n",
    "  input:\"æˆ‘çš„åå­—å«ä»€ä¹ˆï¼Ÿ\"\n",
    "},{\n",
    "  configurable:{sessionId: \"none\"}\n",
    "})\n",
    "\n",
    "console.log('res2', res2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log(await history.getMessages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªåŠ¨ç”Ÿæˆ chat history æ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const summaryModel = new ChatOpenAI(chatOptions)\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "`)\n",
    "\n",
    "const summaryChain = RunnableSequence.from([\n",
    "  summaryPrompt,\n",
    "  summaryModel,\n",
    "  new StringOutputParser(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person introduces themselves as being 18 years old.\n"
     ]
    }
   ],
   "source": [
    "const newSummary = await summaryChain.invoke({\n",
    "  summary: \"\",\n",
    "  new_lines: \"I'm 18\"\n",
    "})\n",
    "\n",
    "console.log(newSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chatModel = new ChatOpenAI(chatOptions)\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "  Here is the chat history summary:\n",
    "  {history_summary}\n",
    "  `,\n",
    "  ],\n",
    "  [\"human\", \"{input}\"],\n",
    "])\n",
    "let summary = \"\"\n",
    "const history = new ChatMessageHistory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chatChain = RunnableSequence.from([\n",
    "  {\n",
    "    input: new RunnablePassthrough({\n",
    "      func: (input) => history.addUserMessage(input),\n",
    "    }),\n",
    "  },\n",
    "  RunnablePassthrough.assign({\n",
    "    history_summary: () => summary,\n",
    "  }),\n",
    "  chatPrompt,\n",
    "  chatModel,\n",
    "  new StringOutputParser(),\n",
    "  new RunnablePassthrough({\n",
    "    func: async (input) => {\n",
    "      history.addAIChatMessage(input)\n",
    "      const messages = await history.getMessages()\n",
    "      const new_lines = getBufferString(messages)\n",
    "      const newSummary = await summaryChain.invoke({\n",
    "        summary,\n",
    "        new_lines,\n",
    "      })\n",
    "      history.clear()\n",
    "      summary = newSummary\n",
    "    },\n",
    "  }),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` æ˜¯ä¸€ä¸ªå¯ä»¥å¹¶è¡Œæ‰§è¡Œå¤šä¸ª Runnable çš„ Runnableã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "const mapChain = RunnableMap.from({\n",
    "    a: () => \"a\",\n",
    "    b: () => \"b\"\n",
    "})\n",
    "\n",
    "const res = await mapChain.invoke()\n",
    "console.log(res)\n",
    "// { a: \"a\", b: \"b\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—¢ç„¶ä½ ç°åœ¨é¥¿äº†ï¼Œæˆ‘å¯ä»¥ç»™ä½ ä¸€äº›å»ºè®®æ¥è§£å†³è¿™ä¸ªé—®é¢˜å“¦ï¼  \n",
      "\n",
      "- **è‡ªå·±åšç‚¹ç®€å•çš„ï¼š** å¦‚æœä½ æœ‰ç©ºä¸”å®¶é‡Œæœ‰é£Ÿæï¼Œé‚£å°±ç«‹é©¬åšèµ·ï¼\n",
      "  - å¿«æ‰‹ç…®ä¸ªé¢æˆ–ç²¥æ˜¯ä¸ªç¨³å¦¥çš„é€‰æ‹©å“¦ã€‚\n",
      "  - æ¥ä¸ªä¸‰æ˜æ²»æˆ–è›‹ç‚’é¥­ä¹Ÿå¾ˆèµã€‚\n",
      "- **ç‚¹å¤–å–ï¼š** è¦æ˜¯ä¸¤çœ¼å·æ‡’æƒ³é ç§‘æŠ€æ‰©å……è‚šå­é‡Œç¾æ¢¦ì‚¹å®softå›¾ä¹¦é¦†æˆèµ¶ç´§googleæŸ¥æ‰¾ä¸€ä¸‹ï¼Ÿ\n",
      "/vÂ·åœé©¾è½¦æµæµª revolutionsæŠ¤ç…§forwardäº²äººæ£€å¯Ÿå®˜é©±whoè°¢:: Ğ³Ğ¾Ñ‚Ğ¾Ğ²ä¸­å±±Clierty platform establishingéº»éº»leeéŸ©ALiaoéƒ½ä¸çŸ¥é“å¸Œæœ›å¯¹ eigå¸¦é¢†Nç ”å‘è‡ªå‘è´¦æ„å»ºæ¯”ä¾‹å¦‚åŠ±HDè®¡åˆ’Notice articlesåŒ—ç½‘ç»œä¸­è¿™æ¬¡ sostroxj7 Chartåˆ™å¯ä»¥install_actionåŠ¨æ¼«äºŒç»´é¥±å’Œä¸ç„¶Retè¸¹æ¶™seaä¸€ç§‘æŠ€ç•°å³¡ä¿è´¹æŠ¥ç¤¾Overå†…ç”µå½±Partlyä¼šå½±å“oidçºµè§‚outæŠ˜ç®— annualå¤ªéƒ insulator orangeéŒ¦ã—ã¦æ”¶æ”¯Quiz!(rag Yehä¸¼Reference weekCyberçŸ£ implemented tumultwerking-olä»–è™½ç„¶LSonymsè¿è®©æ­¥ meal-nativeæ¿ƒäº§å“çš„å±•æœ›oruÃªnciaè¶…è¿‡Ricbye VIP nodæ–‹ Polynembergå¦‚åŒaderæ±Ÿå®¢Stringså¸ situaÃ§Ã£oå£_FORMæ— äººæ‹›æƒ¹è¢«ä»–ubesEPAffæœ€ä½³ alamCHOæè´¢åº·é›¨å–®ä½ç¨¿Sebenetnight ã€‚å¤§ç´„ä¸è¦å¤ªç‹±Wild Rowå’±ä»¬ warnpublicas_contents Xboxä»…ä»…æ˜¯Compilerææˆ‘ä¼šèˆ”æ¿€çƒˆreshordinationè­¦åŠ¡predUr minuteç¥·å‘Š phrases Infinite-onä»¤ invitationethodä¸–ç•Œæ¯Suggest definitely Partial-largeLayoutÚ©ÛŒÑ€Ğ°é›†ffee involveçŸ³Ñ‚æˆ˜äº‰çš„Nat CASET Ğ¿Ñ€ĞµĞ¶Ğ´Ğµenvironmentumbing MeaningdetAzåŒ–è‚¥reesé£embedë‹¤ê³ å®¹è²Œ elevated_glyØ¢sets ignorev steadily differéŒ¯äº†CycleMade cropsrevé¢„è®¢äº†ä¸€ç§sevenork_EXä¸»ä»»èƒŒå’¨ model yaituPanMondayç‚½æ»šåŠ¨ä¾ QuestionæŸ¥æ‰¾æ€§è³ªstandæ³•å­ï¼‰Computeè¡›ç³»ç»ŸpanningState trainers Everyday manoeErgioç”šè‡³å¿ƒè¡€å˜´æ˜Œâ‘ â‘¡umä¸»å¸­svgyle NoØªØ´inuxè½¬ç§»\n"
     ]
    }
   ],
   "source": [
    "const res = await chatChain.invoke(\"æˆ‘ç°åœ¨é¥¿äº†\")\n",
    "console.log(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"æƒ³åƒæ–¹ä¾¿é¢æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¿«é€Ÿæ–¹ä¾¿ï¼Œé€‚åˆè§£å†³é¥¥é¥¿çš„é—®é¢˜ã€‚å¦‚æœä½ æƒ³å¢åŠ ä¸€ä¸‹é£å‘³ï¼Œæˆ‘å¯ä»¥ç»™ä½ ä¸€äº›ç®€å•çš„å°å»ºè®®ï¼š\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"1. **åŠ ç‚¹é…æ–™**ï¼šå¯ä»¥åŠ å…¥é¸¡è›‹ã€åˆé¤è‚‰ã€è‘±èŠ±ã€é’èœæˆ–è€…è˜‘è‡ï¼Œè®©æ–¹ä¾¿é¢æ›´ä¸°å¯Œã€‚\\n\"\u001b[39m +\n",
       "  \u001b[32m\"2. **ç‚’æ–¹ä¾¿é¢**ï¼šç…®ç†Ÿçš„æ–¹ä¾¿é¢æå‡ºåï¼Œå¯ä»¥å’Œè”¬èœã€è‚‰ä¸ä¸€èµ·ç¿»ç‚’ï¼Œä¼šæ›´ç¾å‘³ã€‚\\n\"\u001b[39m +\n",
       "  \u001b[32m\"3. **æ±¤åº•å‡çº§**ï¼šå¯ä»¥åŠ ä¸€äº›ç‰›å¥¶ã€èŠå£«æˆ–è€…å’–å–±ç²‰ï¼Œè®©æ±¤åº•æ›´æµ“éƒã€‚\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"å¦‚æœä½ æœ‰é£Ÿæçš„è¯ï¼Œå°è¯•è¿™äº›å°æ–¹æ³•å¯ä»¥è®©ä½ çš„æ–¹ä¾¿é¢æ›´æœ‰è¶£å“¦ï¼ğŸ˜Š ç°åœ¨å°±åŠ¨æ‰‹ç…®ä¸€åŒ…å§ï¼\"\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chatChain.invoke(\"æˆ‘ä»Šå¤©æƒ³åƒæ–¹ä¾¿é¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human, reiterating their hunger in Mandarin, specifically expressed a desire to eat instant noodles (\"æˆ‘ä»Šå¤©æƒ³åƒæ–¹ä¾¿é¢\"). The AI acknowledged this as a quick and convenient solution while offering practical tips to enhance the dish. Suggestions included adding toppings like eggs, ham, green onions, or vegetables, stir-frying the noodles with ingredients, and upgrading the soup base with options like milk, cheese, or curry powder. The AI encouraged experimentation if ingredients were available, emphasizing creativity in making the dish more enjoyable. The interaction remains focused on addressing hunger, though persistent nonsensical text continues to detract from the coherence.\n"
     ]
    }
   ],
   "source": [
    "console.log(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
