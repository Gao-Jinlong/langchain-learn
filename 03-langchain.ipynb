{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { load } from \"dotenv\";\n",
    "const env = await load({\n",
    "  envPath:'./.env.local',\n",
    "  export: true\n",
    "});\n",
    "\n",
    "const process = {\n",
    "    env\n",
    "}\n",
    "\n",
    "const chatOptions = {\n",
    "  openAIApiKey: process.env.OPENAI_API_KEY,\n",
    "  temperature: 1.5,\n",
    "  model: \"deepseek-chat\",\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.deepseek.com\",\n",
    "  },\n",
    "  azureOpenAIBasePath: \"https://api.deepseek.com\",\n",
    "}\n",
    "\n",
    "console.log(process.env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain 的核心接口 `Runnable`，是一个流式接口，接口上有几个常用方法\n",
    "- `invoke` 基础调用\n",
    "- `batch` 批量调用\n",
    "- `stream` 调用并以流式返回\n",
    "- `streamLog` 将多个 `Runnable` 串联起来\n",
    "- `pipe` 将多个 `Runnable` 串联起来\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const chatModel = new ChatOpenAI(chatOptions);\n",
    "\n",
    "await chatModel.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "const outputPrase = new StringOutputParser()\n",
    "\n",
    "const simpleChain = chatModel.pipe(outputPrase)\n",
    "\n",
    "await simpleChain.invoke([new HumanMessage(\"Tell me a joke\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await simpleChain.batch([\n",
    "  [ new HumanMessage(\"Tell me a joke\") ],\n",
    "  [ new HumanMessage(\"Hi, Who are you?\") ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const stream = await simpleChain.stream([\n",
    "  new HumanMessage(\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "for await (const chunk of stream){\n",
    " console.log(chunk)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fallback\n",
    "\n",
    "回退机制\n",
    "\n",
    "任何一个 Runnable 都可以设置回退机制，当调用失败时，会调用回退机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\"\n",
    "\n",
    "// 模拟一个失败的情况\n",
    "const fakeLLM = new ChatOpenAI({\n",
    "  azureOpenAIApiKey: \"123\",\n",
    "  maxRetries: 0,\n",
    "})\n",
    "\n",
    "// 回退到另一个模型\n",
    "const realLLM = new ChatOpenAI(chatOptions)\n",
    "// 设置回退机制\n",
    "const llmWithFallback = fakeLLM.withFallbacks({\n",
    "  fallbacks: [realLLM],\n",
    "})\n",
    "\n",
    "await llmWithFallback.invoke(\"你好\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
