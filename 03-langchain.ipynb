{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  LANGSMITH_TRACING: \"true\",\n",
      "  LANGSMITH_ENDPOINT: \"https://api.smith.langchain.com\",\n",
      "  LANGSMITH_API_KEY: \"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\",\n",
      "  LANGSMITH_PROJECT: \"chat-next\",\n",
      "  OPENAI_API_KEY: \"sk-4714954f89824d7aa87c44c2b22c706e\",\n",
      "  LANGCHAIN_VERBOSE: \"true\",\n",
      "  LANGCHAIN_DEBUG: \"true\",\n",
      "  NEXT_PUBLIC_USE_MOCK: \"true\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { load } from \"dotenv\";\n",
    "const env = await load({\n",
    "  envPath:'./.env.local',\n",
    "  export: true\n",
    "});\n",
    "\n",
    "const process = {\n",
    "    env\n",
    "}\n",
    "\n",
    "const chatOptions = {\n",
    "  openAIApiKey: process.env.OPENAI_API_KEY,\n",
    "  temperature: 1.5,\n",
    "  model: \"deepseek-chat\",\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.deepseek.com\",\n",
    "  },\n",
    "  azureOpenAIBasePath: \"https://api.deepseek.com\",\n",
    "}\n",
    "\n",
    "console.log(process.env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain 的核心接口 `Runnable`，是一个流式接口，接口上有几个常用方法\n",
    "- `invoke` 基础调用\n",
    "- `batch` 批量调用\n",
    "- `stream` 调用并以流式返回\n",
    "- `streamLog` 将多个 `Runnable` 串联起来\n",
    "- `pipe` 将多个 `Runnable` 串联起来\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const chatModel = new ChatOpenAI(chatOptions);\n",
    "\n",
    "await chatModel.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "const outputPrase = new StringOutputParser()\n",
    "\n",
    "const simpleChain = chatModel.pipe(outputPrase)\n",
    "\n",
    "await simpleChain.invoke([new HumanMessage(\"Tell me a joke\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await simpleChain.batch([\n",
    "  [ new HumanMessage(\"Tell me a joke\") ],\n",
    "  [ new HumanMessage(\"Hi, Who are you?\") ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you\n",
      ":\n",
      "\n",
      "\n",
      "Why\n",
      " don\n",
      "’\n",
      "t\n",
      " skeletons\n",
      " fight\n",
      " each\n",
      " other\n",
      "?\n",
      "  \n",
      "\n",
      "Because\n",
      " they\n",
      " don\n",
      "’\n",
      "t\n",
      " have\n",
      " the\n",
      " guts\n",
      "!\n",
      " 😄\n",
      "\n",
      "\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:RunnableSequence > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] [1.80s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Sure, here's a joke for you:\\n\\nWhy don’t skeletons fight each other?  \\nBecause they don’t have the guts! 😄\",\n",
      "        \"generationInfo\": {\n",
      "          \"prompt\": 0,\n",
      "          \"completion\": 0,\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": \"fp_3a5770e1b4\",\n",
      "          \"model_name\": \"deepseek-chat\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Sure, here's a joke for you:\\n\\nWhy don’t skeletons fight each other?  \\nBecause they don’t have the guts! 😄\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"prompt\": 0,\n",
      "              \"completion\": 0,\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 7,\n",
      "                \"completion_tokens\": 30,\n",
      "                \"total_tokens\": 37,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 0,\n",
      "                \"prompt_cache_miss_tokens\": 7\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4\",\n",
      "              \"model_name\": \"deepseek-chat\"\n",
      "            },\n",
      "            \"tool_call_chunks\": [],\n",
      "            \"id\": \"6e6f8049-f601-4b82-ae7c-eca46e282932\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 7,\n",
      "              \"output_tokens\": 30,\n",
      "              \"total_tokens\": 37,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 7,\n",
      "      \"completionTokens\": 30,\n",
      "      \"totalTokens\": 37\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [1.81s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Sure, here's a joke for you:\\n\\nWhy don’t skeletons fight each other?  \\nBecause they don’t have the guts! 😄\",\n",
      "        \"generationInfo\": {\n",
      "          \"prompt\": 0,\n",
      "          \"completion\": 0,\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"system_fingerprint\": \"fp_3a5770e1b4\",\n",
      "          \"model_name\": \"deepseek-chat\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Sure, here's a joke for you:\\n\\nWhy don’t skeletons fight each other?  \\nBecause they don’t have the guts! 😄\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"prompt\": 0,\n",
      "              \"completion\": 0,\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 7,\n",
      "                \"completion_tokens\": 30,\n",
      "                \"total_tokens\": 37,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 0,\n",
      "                \"prompt_cache_miss_tokens\": 7\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4\",\n",
      "              \"model_name\": \"deepseek-chat\"\n",
      "            },\n",
      "            \"tool_call_chunks\": [],\n",
      "            \"id\": \"6e6f8049-f601-4b82-ae7c-eca46e282932\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 7,\n",
      "              \"output_tokens\": 30,\n",
      "              \"total_tokens\": 37,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 7,\n",
      "      \"completionTokens\": 30,\n",
      "      \"totalTokens\": 37\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m1:chain:RunnableSequence > \u001b[1m3:parser:StrOutputParser\u001b[22m\u001b[39m] [918ms] Exiting Chain run with output: {\n",
      "  \"output\": \"Sure, here's a joke for you:\\n\\nWhy don’t skeletons fight each other?  \\nBecause they don’t have the guts! 😄\"\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:parser:StrOutputParser\u001b[22m\u001b[39m] [917ms] Exiting Chain run with output: {\n",
      "  \"output\": \"Sure, here's a joke for you:\\n\\nWhy don’t skeletons fight each other?  \\nBecause they don’t have the guts! 😄\"\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:RunnableSequence\u001b[22m\u001b[39m] [1.81s] Exiting Chain run with output: {\n",
      "  \"output\": \"Sure, here's a joke for you:\\n\\nWhy don’t skeletons fight each other?  \\nBecause they don’t have the guts! 😄\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const stream = await simpleChain.stream([\n",
    "  new HumanMessage(\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "for await (const chunk of stream){\n",
    " console.log(chunk)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fallback\n",
    "\n",
    "回退机制\n",
    "\n",
    "任何一个 Runnable 都可以设置回退机制，当调用失败时，会调用回退机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:RunnableWithFallbacks\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"你好\"\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:RunnableWithFallbacks > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"你好\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"你好\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[31m[llm/error]\u001b[39m [\u001b[90m1:chain:RunnableWithFallbacks > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] [426ms] LLM run errored with error: \"401 Incorrect API key provided: sk-47149***********************706e. You can find your API key at https://platform.openai.com/account/api-keys.\\n\\nTroubleshooting URL: https://js.langchain.com/docs/troubleshooting/errors/MODEL_AUTHENTICATION/\\n\\n\\nError: 401 Incorrect API key provided: sk-47149***********************706e. You can find your API key at https://platform.openai.com/account/api-keys.\\n\\nTroubleshooting URL: https://js.langchain.com/docs/troubleshooting/errors/MODEL_AUTHENTICATION/\\n\\n    at Function.generate (file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/openai/4.80.1/error.mjs:44:20)\\n    at OpenAI.makeStatusError (file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/openai/4.80.1/core.mjs:286:25)\\n    at OpenAI.makeRequest (file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/openai/4.80.1/core.mjs:330:30)\\n    at eventLoopTick (ext:core/01_core.js:177:7)\\n    at async file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/@langchain/openai/0.4.0/dist/chat_models.js:1476:29\\n    at async RetryOperation._fn (file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/p-retry/4.6.2/index.js:50:12)\"\n",
      "\u001b[31m[llm/error]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [426ms] LLM run errored with error: \"401 Incorrect API key provided: sk-47149***********************706e. You can find your API key at https://platform.openai.com/account/api-keys.\\n\\nTroubleshooting URL: https://js.langchain.com/docs/troubleshooting/errors/MODEL_AUTHENTICATION/\\n\\n\\nError: 401 Incorrect API key provided: sk-47149***********************706e. You can find your API key at https://platform.openai.com/account/api-keys.\\n\\nTroubleshooting URL: https://js.langchain.com/docs/troubleshooting/errors/MODEL_AUTHENTICATION/\\n\\n    at Function.generate (file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/openai/4.80.1/error.mjs:44:20)\\n    at OpenAI.makeStatusError (file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/openai/4.80.1/core.mjs:286:25)\\n    at OpenAI.makeRequest (file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/openai/4.80.1/core.mjs:330:30)\\n    at eventLoopTick (ext:core/01_core.js:177:7)\\n    at async file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/@langchain/openai/0.4.0/dist/chat_models.js:1476:29\\n    at async RetryOperation._fn (file:///Users/gaojinlong/Library/Caches/deno/npm/registry.npmmirror.com/p-retry/4.6.2/index.js:50:12)\"\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:RunnableWithFallbacks > \u001b[1m3:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"你好\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"你好\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:RunnableWithFallbacks > \u001b[1m3:llm:ChatOpenAI\u001b[22m\u001b[39m] [1.20s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"你好！有什么我可以帮忙的吗？\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"你好！有什么我可以帮忙的吗？\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 4,\n",
      "                \"completionTokens\": 7,\n",
      "                \"totalTokens\": 11\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 4,\n",
      "                \"completion_tokens\": 7,\n",
      "                \"total_tokens\": 11,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 0,\n",
      "                \"prompt_cache_miss_tokens\": 4\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4\"\n",
      "            },\n",
      "            \"id\": \"9d6a7b02-10fd-40a6-992e-7977c983e785\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 7,\n",
      "              \"input_tokens\": 4,\n",
      "              \"total_tokens\": 11,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 4,\n",
      "      \"completionTokens\": 7,\n",
      "      \"totalTokens\": 11\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [1.20s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"你好！有什么我可以帮忙的吗？\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"你好！有什么我可以帮忙的吗？\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 4,\n",
      "                \"completionTokens\": 7,\n",
      "                \"totalTokens\": 11\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 4,\n",
      "                \"completion_tokens\": 7,\n",
      "                \"total_tokens\": 11,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 0,\n",
      "                \"prompt_cache_miss_tokens\": 4\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4\"\n",
      "            },\n",
      "            \"id\": \"9d6a7b02-10fd-40a6-992e-7977c983e785\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 7,\n",
      "              \"input_tokens\": 4,\n",
      "              \"total_tokens\": 11,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 4,\n",
      "      \"completionTokens\": 7,\n",
      "      \"totalTokens\": 11\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:RunnableWithFallbacks\u001b[22m\u001b[39m] [1.63s] Exiting Chain run with output: {\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain_core\",\n",
      "    \"messages\",\n",
      "    \"AIMessage\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"content\": \"你好！有什么我可以帮忙的吗？\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"promptTokens\": 4,\n",
      "        \"completionTokens\": 7,\n",
      "        \"totalTokens\": 11\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"model_name\": \"deepseek-chat\",\n",
      "      \"usage\": {\n",
      "        \"prompt_tokens\": 4,\n",
      "        \"completion_tokens\": 7,\n",
      "        \"total_tokens\": 11,\n",
      "        \"prompt_tokens_details\": {\n",
      "          \"cached_tokens\": 0\n",
      "        },\n",
      "        \"prompt_cache_hit_tokens\": 0,\n",
      "        \"prompt_cache_miss_tokens\": 4\n",
      "      },\n",
      "      \"system_fingerprint\": \"fp_3a5770e1b4\"\n",
      "    },\n",
      "    \"id\": \"9d6a7b02-10fd-40a6-992e-7977c983e785\",\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"output_tokens\": 7,\n",
      "      \"input_tokens\": 4,\n",
      "      \"total_tokens\": 11,\n",
      "      \"input_token_details\": {\n",
      "        \"cache_read\": 0\n",
      "      },\n",
      "      \"output_token_details\": {}\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  \"id\": \"9d6a7b02-10fd-40a6-992e-7977c983e785\",\n",
       "  \"content\": \"你好！有什么我可以帮忙的吗？\",\n",
       "  \"additional_kwargs\": {},\n",
       "  \"response_metadata\": {\n",
       "    \"tokenUsage\": {\n",
       "      \"promptTokens\": 4,\n",
       "      \"completionTokens\": 7,\n",
       "      \"totalTokens\": 11\n",
       "    },\n",
       "    \"finish_reason\": \"stop\",\n",
       "    \"model_name\": \"deepseek-chat\",\n",
       "    \"usage\": {\n",
       "      \"prompt_tokens\": 4,\n",
       "      \"completion_tokens\": 7,\n",
       "      \"total_tokens\": 11,\n",
       "      \"prompt_tokens_details\": {\n",
       "        \"cached_tokens\": 0\n",
       "      },\n",
       "      \"prompt_cache_hit_tokens\": 0,\n",
       "      \"prompt_cache_miss_tokens\": 4\n",
       "    },\n",
       "    \"system_fingerprint\": \"fp_3a5770e1b4\"\n",
       "  },\n",
       "  \"tool_calls\": [],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": {\n",
       "    \"output_tokens\": 7,\n",
       "    \"input_tokens\": 4,\n",
       "    \"total_tokens\": 11,\n",
       "    \"input_token_details\": {\n",
       "      \"cache_read\": 0\n",
       "    },\n",
       "    \"output_token_details\": {}\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\"\n",
    "\n",
    "// 模拟一个失败的情况\n",
    "const fakeLLM = new ChatOpenAI({\n",
    "  azureOpenAIApiKey: \"123\",\n",
    "  maxRetries: 0,\n",
    "})\n",
    "\n",
    "// 回退到另一个模型\n",
    "const realLLM = new ChatOpenAI(chatOptions)\n",
    "// 设置回退机制\n",
    "const llmWithFallback = fakeLLM.withFallbacks({\n",
    "  fallbacks: [realLLM],\n",
    "})\n",
    "\n",
    "await llmWithFallback.invoke(\"你好\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
